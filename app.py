# app.py
# ×§×•×‘×¥ Flask ×¨××©×™ ×¢× ×œ×•×’×™×§×ª Embeddings ×•×—×™×¤×•×©

import os
import re
import unicodedata
import copy
from dataclasses import dataclass
from typing import List, Optional, Dict, Any

# ×™×™×‘×•× ×¡×¤×¨×™×•×ª Flask ×•-Jinja2
from flask import Flask, render_template, request, jsonify

# ×™×™×‘×•× ×¡×¤×¨×™×•×ª ×”-AI ×•×”×—×™×¤×•×© ×©×œ×š
from rapidfuzz import fuzz
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# ============================================
# ×”×’×“×¨×•×ª Flask ×•×˜×¢×™× ×ª × ×ª×•× ×™× ×’×œ×•×‘×œ×™×™×
# ============================================

app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False # ×—×©×•×‘ ×œ×ª×¦×•×’×” ×ª×§×™× ×” ×©×œ ×¢×‘×¨×™×ª ×‘-JSON

# ×”×’×“×¨×ª ××©×ª× ×™× ×’×œ×•×‘×œ×™×™× (×™×˜×¢× ×• ×¤×¢× ××—×ª)
FAQ_PATH = "faq.txt" # ×§×•×‘×¥ ×–×” ×¦×¨×™×š ×œ×”×™×•×ª ×‘×ª×™×§×™×™×ª ×”×‘×¡×™×¡
faq_items = []
faq_store = None
embeddings = None
embeddings_ready = False
openai_api_key = None

# ×”×’×“×¨×ª ×©××œ×•×ª × ×¤×•×¦×•×ª ×œ×”×¦×’×” ×‘×“×£ ×”×‘×™×ª
POPULAR_FAQ_LIST = [
    "××™×š ××•×¡×™×¤×™× ××©×ª××© ×—×“×© ×‘××ª×¨ ××™×™×¦×’×™×.",
    "××§×‘×œ ×”×•×“×¢×” ×©××—×“ ××• ×™×•×ª×¨ ×× ×ª×•× ×™ ×”×”×–×“×”×•×ª ×©×’×•×™×™×.",
    "××™×š ×™×•×¦×¨×™× ×§×™×¦×•×¨ ×“×¨×š ×œ××ª×¨ ××™×™×¦×’×™× ×¢×œ ×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”.",
    "×¨×•×¦×” ×œ×§×‘×œ ××ª ×”×§×•×“ ×”×—×“ ×¤×¢××™ ×œ×“×•××¨ ××œ×§×˜×¨×•× ×™.",
]

# ============================================
# ×œ×•×’×™×§×ª ×¢×™×‘×•×“ ×•×—×™×¤×•×© (××•×¢×ª×§×ª ××”×§×•×“ ×©×œ×š)
# ============================================

@dataclass
class FAQItem:
    question: str
    variants: List[str]
    answer: str

def normalize_he(s: str) -> str:
    """×× ×§×”, ×××—×“, ×•××¨×—×™×‘ × ×™×¡×•×—×™× ×—×œ×§×™×™× ×œ×©×¤×” ×˜×‘×¢×™×ª."""
    if not s:
        return ""
    # ... ×›××Ÿ × ×›× ×¡ ×”×§×•×“ ×”××œ× ×©×œ normalize_he ...
    s = unicodedata.normalize("NFC", s)
    s = re.sub(r"[\u200e\u200f]", "", s)
    s = re.sub(r"[^\w\s\u0590-\u05FF]", " ", s)
    s = re.sub(r"\s+", " ", s).strip().lower()

    patterns = [
        (r"^×¨×•×¦×” ", "××™×š "),
        (r"^×× ×™ ×¨×•×¦×” ", "××™×š "),
        (r"^××¤×©×¨ ", "××™×š "),
        (r"^×‘× ×œ×™ ", "××™×š "),
        (r"^××‘×§×© ", "××™×š "),
    ]
    for p, repl in patterns:
        if re.match(p, s):
            s = re.sub(p, repl, s)
            break

    s = s.replace("×¢××“×”", "×¢××“×” ×œ××—×©×‘")
    s = s.replace("×œ×”×•×¡×™×£ ×¢××“×”", "×œ×”×•×¡×™×£ ××©×ª××© ×—×“×©")

    return s

def parse_faq_new(text: str) -> List[FAQItem]:
    """××¤×¨×§ ××ª ×”×§×•×‘×¥ ×œ×¤×™ ××‘× ×” ×©××œ×”/× ×™×¡×•×—×™×/×ª×©×•×‘×”"""
    items = []
    blocks = re.split(r"(?=×©××œ×”\s*:)", text)
    for b in blocks:
        b = b.strip()
        if not b:
            continue
        # ... ×›××Ÿ × ×›× ×¡ ×§×•×“ ×”×¤××¨×¡×™× ×’ ×”××œ× ×©×œ×š ...
        q_match = re.search(r"×©××œ×”\s*:\s*(.+)", b)
        a_match = re.search(r"(?s)×ª×©×•×‘×”\s*:\s*(.+?)(?:\n×”×•×¨××”\s*:|\Z)", b)
        v_match = re.search(r"(?s)× ×™×¡×•×—×™× ×“×•××™×\s*:\s*(.+?)(?:\n×ª×©×•×‘×”\s*:|\Z)", b)

        question = q_match.group(1).strip() if q_match else ""
        answer = a_match.group(1).strip() if a_match else ""
        variants = []

        if v_match:
            raw = v_match.group(1)
            variants = [s.strip(" -\t") for s in raw.split("\n") if s.strip()]

        items.append(FAQItem(question, variants, answer))
    return items

def search_faq(query: str) -> Dict[str, Any]:
    """
    ××‘×¦×¢ ×—×™×¤×•×© ×¤××–×™ ×•×¡×× ×˜×™. 
    ××—×–×™×¨ ××™×œ×•×Ÿ ×¢× ×”×ª×©×•×‘×” ×•×”×©××œ×•×ª ×”×§×©×•×¨×•×ª.
    """
    global faq_store, faq_items, embeddings_ready, embeddings
    nq = normalize_he(query)

    # ... ×›×œ ×œ×•×’×™×§×ª ×”×—×™×¤×•×© ×”×¤××–×™, ×–×™×”×•×™ ×›×•×•× ×” ×•-Embeddings ...
    verbs = {
        "add": ["×”×•×¡×£", "×œ×”×•×¡×™×£", "×”×•×¡×¤×”", "××•×¡×™×£", "××•×¡×™×¤×™×", "×œ×¦×¨×£", "×¦×™×¨×•×£", "×¤×ª×™×—×”", "×¤×ª×™×—×ª", "×¨×™×©×•×", "×œ×”×™×¨×©×"],
        "delete": ["××—×§", "××—×™×§×”", "×œ×”×¡×™×¨", "×”×¡×¨", "×”×¡×¨×”", "×‘×™×˜×•×œ", "×œ×‘×˜×œ", "×¡×’×•×¨", "×œ×¡×’×•×¨", "×‘×™×˜×•×œ ××©×ª××©"],
        "update": ["×¢×“×›×Ÿ", "×œ×¢×“×›×Ÿ", "×¢×“×›×•×Ÿ", "×©×™× ×•×™", "×œ×©× ×•×ª", "×¢×¨×™×›×”", "×¢×¨×•×š", "×œ×ª×§×Ÿ", "×ª×™×§×•×Ÿ"]
    }
    intent = None
    for k, words in verbs.items():
        if any(w in nq for w in words):
            intent = k
            break
    
    scored = []
    for i, item in enumerate(faq_items):
        all_texts = [item.question] + item.variants
        for t in all_texts:
            score = fuzz.token_sort_ratio(nq, normalize_he(t))

            t_intent = None
            for k, words in verbs.items():
                if any(w in t for w in words):
                    t_intent = k
                    break

            if intent and t_intent and intent != t_intent:
                score -= 50
            if intent and t_intent and intent == t_intent:
                score += 25

            scored.append((score, i, t.strip(), t_intent))

    scored.sort(reverse=True, key=lambda x: x[0])
    top = scored[:5]

    best_fuzzy_score = top[0][0] if top else 0
    result_item = None
    similar_questions = []

    # ============================
    # Embeddings (×¨×§ ×× ×˜×¢×™× ×” ×”×¦×œ×™×—×”)
    # ============================
    if embeddings_ready and faq_store:
        hits = faq_store.similarity_search_with_score(nq, k=8)
        key_words = ["×™×¤×•×™", "×›×•×—", "×”×¨×©××”", "×™×™×¦×•×’", "××™×™×¦×’", "××¢×¡×™×§", "××‘×•×˜×—"]
        boosted_hits = []

        for doc, score in hits:
            idx = doc.metadata["idx"]
            question_text = faq_items[idx].question
            text_norm = normalize_he(question_text + " " + " ".join(faq_items[idx].variants))

            for kw in key_words:
                if kw in nq and kw in text_norm:
                    score -= 0.15
            boosted_hits.append((doc, score))

        boosted_hits.sort(key=lambda x: x[1])
        best_embed_score = boosted_hits[0][1] if boosted_hits else 999
        
        # ×ª× ××™ ×¡×£ ×¨×œ×•×•× ×˜×™×•×ª
        if best_fuzzy_score < 55 and best_embed_score > 1.2:
             return {"success": False, "answer": "×œ× × ××¦××” ×ª×©×•×‘×”, × ×¡×” ×œ× ×¡×— ××ª ×”×©××œ×” ××—×“×©.", "similar_questions": []}

        if best_fuzzy_score >= 55:
            result_item = copy.deepcopy(faq_items[top[0][1]])
        elif boosted_hits:
            result_item = copy.deepcopy(faq_items[boosted_hits[0][0].metadata["idx"]])

        if result_item:
            similar_questions = [
                faq_items[d.metadata["idx"]].question
                for d, s in boosted_hits[1:4]
                if s <= 1.3 and faq_items[d.metadata["idx"]].question.strip() != result_item.question.strip()
            ][:3]
    
    # ×× ×”-embeddings ×œ× ×¢×‘×“×• (×‘×’×œ×œ ×©×’×™××ª ×˜×¢×™× ×”) × ×¡×ª××š ×¨×§ ×¢×œ ×¤××–×™
    elif best_fuzzy_score >= 55:
        result_item = copy.deepcopy(faq_items[top[0][1]])
        
    if not result_item:
        return {"success": False, "answer": "×œ× × ××¦××” ×ª×©×•×‘×”, × ×¡×” ×œ× ×¡×— ××ª ×”×©××œ×” ××—×“×©.", "similar_questions": []}

    # ×¢×™×¦×•×‘ ×”×ª×•×¦××” ×”×¡×•×¤×™×ª
    answer_text = result_item.answer.strip()
    
    # ×”×•×¡×¤×ª ×›×•×ª×¨×•×ª ××˜× ×“××˜×” ×œ×©× ×‘×“×™×§×” (×›×“×™ ×©×ª×•×›×œ ×œ×¨××•×ª ××” ×–×•×”×”)
    answer_text += f"\n\n--- ××˜× ×“××˜×” ---\n××§×•×¨: faq\n×©××œ×” ××–×•×”×”: {result_item.question}"
    
    return {
        "success": True, 
        "answer": answer_text,
        "similar_questions": similar_questions
    }

# ============================================
# ×˜×¢×™× ×” ×¨××©×•× ×™×ª ×©×œ ×”××•×“×œ×™× (×¤×•× ×§×¦×™×” ×¨×¦×” ×¤×¢× ××—×ª!)
# ============================================

def load_initial_data():
    global faq_items, faq_store, embeddings_ready, embeddings, openai_api_key
    
    # 1. ××¤×ª×— API: Render ××›× ×™×¡ ××ª ×–×” ×›××©×ª× ×” ×¡×‘×™×‘×”
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    if not openai_api_key:
        print("ğŸš¨ ××–×”×¨×”: ××©×ª× ×” OPENAI_API_KEY ×œ× ×”×•×’×“×¨. ×”×—×™×¤×•×© ×”×¡×× ×˜×™ (Embeddings) ×œ× ×™×¢×‘×•×“.")
    
    # 2. ×˜×¢×™× ×ª ×§×•×‘×¥ ×”-FAQ
    try:
        with open(FAQ_PATH, "r", encoding="utf-8") as f:
            raw_faq = f.read()
    except FileNotFoundError:
        print(f"âŒ ×©×’×™××” ×—××•×¨×”: ×§×•×‘×¥ FAQ ×œ× × ××¦× ×‘× ×ª×™×‘: {FAQ_PATH}.")
        return

    faq_items = parse_faq_new(raw_faq)
    print(f"âœ… × ×˜×¢× ×• {len(faq_items)} ×©××œ×•×ª ××”-FAQ.")
    
    # 3. ×™×¦×™×¨×ª Embeddings ×•-FAISS
    if openai_api_key and faq_items:
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
            docs = [Document(page_content=" | ".join([item.question] + item.variants), metadata={"idx": i})
                    for i, item in enumerate(faq_items)]
            faq_store = FAISS.from_documents(docs, embeddings)
            embeddings_ready = True
            print("âœ… ××™× ×“×§×¡ embeddings × ×•×¦×¨ ×‘×”×¦×œ×—×”.")
        except Exception as e:
            print(f"âŒ ×©×’×™××”: ×˜×¢×™× ×ª FAISS/Embeddings × ×›×©×œ×”: {e}. ×¨×¥ ×¨×§ ×‘××¦×‘ ×¤××–×™.")
            embeddings_ready = False

# ×”×¨×¦×ª ×”×˜×¢×™× ×” ×¤×¢× ××—×ª ×œ×¤× ×™ ×”× ×™×ª×•×‘×™×
with app.app_context():
    load_initial_data()

# ============================================
# × ×™×ª×•×‘×™× (Routes) ×©×œ Flask
# ============================================

@app.route('/')
def index():
    # × ×™×ª×•×‘ ×œ×“×£ ×”×‘×™×ª - ××¦×™×’ ××ª ×ª×‘× ×™×ª HTML
    
    # ××¢×‘×™×¨ ××ª ×¨×©×™××ª ×”×©××œ×•×ª ×”× ×¤×•×¦×•×ª ×”×§×‘×•×¢×” ×œ×ª×‘× ×™×ª Jinja2
    return render_template('index.html', popular_questions=POPULAR_FAQ_LIST)

@app.route('/search', methods=['POST'])
def search():
    # × ×™×ª×•×‘ ×œ×˜×™×¤×•×œ ×‘×‘×§×©×•×ª ×—×™×¤×•×© ×‘×××¦×¢×•×ª AJAX / JavaScript
    data = request.get_json()
    query = data.get('query', '')
    
    if not query:
        return jsonify({"success": False, "answer": "×©××™×œ×ª×” ×¨×™×§×”."})
    
    # ×”×¤×¢×œ×ª ×œ×•×’×™×§×ª ×”×—×™×¤×•×©
    result = search_faq(query)
    
    # ×”×—×–×¨×ª ×”×ª×•×¦××” ×›-JSON
    return jsonify(result)
